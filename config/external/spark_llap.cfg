# Use lower case for the variable names!
# The way jinja2 works forces to do this atm

[global]
mysql_connector_version = 5.1.46

hadoop_version = 3.1.1
# hadoop_version = 3.2.0-SNAPSHOT

hive_version = 4.0.0-SNAPSHOT
tez_version = 0.10.1-SNAPSHOT

hive_path = /Users/jmarhuenda/workspace/hive
tez_path = /Users/jmarhuenda/workspace/tez

# zookeeper configuration folder, don't modify
zookeeper_conf = conf
zookeeper_scripts = scripts

[core]

# Getting weird error: ERROR ApiServiceClient: User: yarn/rm.example.com@EXAMPLE.COM is not allowed to impersonate dr.who
hadoop.proxyuser.yarn.hosts = *
hadoop.proxyuser.yarn.groups = *
hadoop.http.staticuser.user = hive
# hadoop.kms.authentication.zk-dt-secret-manager.zkConnectionString = zk1.example.com:2181
# templeton.zookeeper.hosts = zk1.example.com:2181

[hdfs]


[yarn]

yarn.webapp.api-service.enable = true

# yarn.resourcemanager.zk-address = zk1.example.com:2181
# yarn.resourcemanager.placement-constraints.handler = scheduler

# If we set this two we have to modify the Yarnfile to make sure
# llap-DDMMMYYYY.tar.gz lands in the appropriate path
# yarn.service.framework.path = /user/yarn/framework
# yarn.service.base.path = /user/yarn/yarn

hadoop.registry.zk.quorum = zk1.example.com:2181
hadoop.registry.secure = true
hadoop.registry.system.acls = sasl:yarn@

yarn.resourcemanager.scheduler.class = org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler

yarn.scheduler.capacity.root.queues=default,llap
yarn.scheduler.capacity.default.minimum-user-limit-percent=100
yarn.scheduler.capacity.maximum-am-resource-percent=100
yarn.scheduler.capacity.maximum-applications=10000
yarn.scheduler.capacity.node-locality-delay=40
yarn.scheduler.capacity.root.accessible-node-labels=*
yarn.scheduler.capacity.root.acl_administer_queue=yarn
yarn.scheduler.capacity.root.capacity=100
yarn.scheduler.capacity.root.default.acl_administer_jobs=yarn
yarn.scheduler.capacity.root.default.acl_submit_applications=yarn
yarn.scheduler.capacity.root.default.capacity=50
yarn.scheduler.capacity.root.default.maximum-capacity=50
yarn.scheduler.capacity.root.default.state=RUNNING
yarn.scheduler.capacity.root.default.user-limit-factor=2
yarn.scheduler.capacity.queue-mappings-override.enable=false
yarn.scheduler.capacity.root.acl_administer_jobs=yarn
yarn.scheduler.capacity.root.default.acl_administer_queue=yarn
yarn.scheduler.capacity.root.llap.acl_administer_queue=hive
yarn.scheduler.capacity.root.llap.acl_submit_applications=hive
yarn.scheduler.capacity.root.llap.capacity=50
yarn.scheduler.capacity.root.llap.maximum-am-resource-percent=1
yarn.scheduler.capacity.root.llap.maximum-capacity=50
yarn.scheduler.capacity.root.llap.minimum-user-limit-percent=100
yarn.scheduler.capacity.root.llap.ordering-policy=fifo
yarn.scheduler.capacity.root.llap.priority=10
yarn.scheduler.capacity.root.llap.state=RUNNING
yarn.scheduler.capacity.root.llap.user-limit-factor=2
yarn.scheduler.capacity.root.ordering-policy=priority-utilization


mapreduce.job.queuename = default
mapreduce.framework.name = yarn
service_check.queue.name = default
yarn.resourcemanager.monitor.capacity.preemption.natural_termination_factor = 1
yarn.resourcemanager.placement-constraints.handler = scheduler

[tez]
tez.am.resource.memory.mb = 256
tez.queue.name = llap
tez.am.node-blacklisting.enabled = false
tez.am.task.listener.thread-count = 1

[hive]

templeton.hadoop.queue.name = default

hive.server2.enable.doAs = true
hive.warehouse.subdir.inherit.perms = true
hive.metastore.kerberos.principal = hive/_HOST@EXAMPLE.COM
hive.server2.tez.default.queues = default,llap
hive.server2.tez.sessions.per.default.queue = 4

hadoop.bin.path=/hadoop/bin/hadoop
hadoop.conf.path=/hadoop/etc/hadoop/

# hive.execution.mode = llap
hive.execution.mode = llap
hive.llap.execution.mode = all
hive.llap.daemon.service.hosts = @dhive-llap
hive.llap.auto.allow.uber = false
hive.llap.daemon.memory.per.instance.mb = 2048
hive.llap.daemon.num.executors = 4
hive.llap.daemon.task.scheduler.enable.preemption = true
hive.llap.io.enabled = false
hive.llap.io.threadpool.size = 20
hive.llap.task.scheduler.locality.delay = -1
hive.llap.task.scheduler.num.schedulable.tasks.per.node = -1
hive.llap.daemon.queue.name = llap

hive.llap.daemon.yarn.container.mb = 4096
hive.llap.daemon.vcpus.per.instance = 4
hive.llap.io.allocator.alloc.min = 512
hive.llap.io.allocator.alloc.max = 2048
hive.llap.io.memory.size = 2048
hive.llap.task.scheduler.timeout.seconds = 60s
hive.llap.object.cache.enabled = true
hive.llap.io.use.lrfu = true
hive.llap.enable.grace.join.in.llap = false
hive.llap.client.consistent.splits = true

dfs.short.circuit.shared.memory.watcher.interrupt.check.ms = 0
dfs.client.mmap.enabled = false

hive.llap.daemon.work.dirs = /tmp/
hive.llap.daemon.keytab.file = /var/keytabs/hdfs.keytab
hive.llap.daemon.service.principal = hive/_HOST@EXAMPLE.COM
hive.llap.task.keytab.file = /var/keytabs/hdfs.keytab
hive.llap.task.principal = hive/_HOST@EXAMPLE.COM

hive.llap.daemon.service.ssl = false

hive.llap.zk.sm.principal = hive/zk1.example.com@EXAMPLE.COM
hive.llap.zk.sm.keytab.file = /var/keytabs/hdfs.keytab
hive.llap.zk.sm.connectionString = zk1.example.com:2181
hive.zookeeper.quorum = zk1.example.com:2181
hive.cluster.delegation.token.store.zookeeper.connectString = zk1.example.com:2181

zk-dt-secret-manager.zkConnectionString = zk1.example.com:2181
hive.cluster.delegation.token.store.zookeeper.acl = sasl:hive/zk1.example.com@EXAMPLE.COM:cdrwa
# hive.zookeeper.client.port = 2181
# templeton.zookeeper.hosts = zk1.example.com:2181

hive.llap.task.scheduler.am.registry.principal = hive/zk1.example.com@EXAMPLE.COM
hive.llap.task.scheduler.am.registry.keytab.file = /var/keytabs/hdfs.keytab

hive.execution.engine = tez
hive.support.concurrency = true
hive.enforce.bucketing  = true
hive.exec.dynamic.partition.mode = nonstrict
hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager
hive.compactor.initiator.on = true
hive.compactor.worker.threads = 1

# spark.yarn.jar = hdfs://nn.example.com:9000/user/hive/spark-2.3.1-bin-hadoop2-without-hive.tgz
# Try commenting spark jars
spark.yarn.jars = hdfs://nn.example.com:9000/user/spark_jars/*


[hivemetastore]

hive.security.metastore.authorization.manager = org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider
hive.security.metastore.authenticator.manager = org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator
hive.metastore.pre.event.listeners = org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener
hive.metastore.execute.setugi = true

hive.warehouse.subdir.inherit.perms = true
# hive.metastore.client.capability.check = false
# hive.users.in.admin.role = hive,hive_meta

# hive.stats.column.autogather = true
# hive.insert.into.multilevel.dirs = true

[services]
# Order is important
# The module file will be kerberos.yml, hadoop.yml, ...
services = kerberos,hadoop,yarn,tez,mysql,hive-meta,hive,llap,zookeeper,external_spark


[external_spark]
hash = #
spark_version = 2.3.1
spark_ball = spark-${spark_version}-bin-hadoop2-without-hive.tgz
scala_file = stream_and_read.scala

# Lines to add the Dockerfile
docker =
    ADD ${spark_ball} /
    COPY ${spark_ball} /
    ADD hive-warehouse-connector-assembly-1.0.0-SNAPSHOT.jar /
    ADD ${scala_file} /
    RUN ln -s spark-${spark_version}-bin-hadoop2-without-hive spark
    ENV SPARK_HOME=/spark
    ENV SPARK_DIST_CLASSPATH=/hadoop/etc/hadoop:/hadoop/share/hadoop/common/lib/*:/hadoop/share/hadoop/common/*:/hadoop/share/hadoop/hdfs:/hadoop/share/hadoop/hdfs/lib/*:/hadoop/share/hadoop/hdfs/*:/hadoop/share/hadoop/mapreduce/*:/hadoop/share/hadoop/yarn:/hadoop/share/hadoop/yarn/lib/*:/hadoop/share/hadoop/yarn/*
    RUN chown -R -L hdfs $$SPARK_HOME
    RUN mkdir $$SPARK_HOME/tmp
    # This jars will be used by hive
    RUN cp /spark/jars/spark-core_2.11-${spark_version}.jar /hadoop/share/hadoop/tools/lib
    RUN cp /spark/jars/spark-sql_2.11-${spark_version}.jar /hadoop/share/hadoop/tools/lib
    RUN cp /spark/jars/spark-streaming_2.11-${spark_version}.jar /hadoop/share/hadoop/tools/lib
    RUN cp /spark/jars/scala-library-2.11.8.jar /hadoop/share/hadoop/tools/lib
    RUN cp /spark/jars/spark-network-common_2.11-${spark_version}.jar /hadoop/share/hadoop/tools/lib
    # RUN cp /hive/lib/hive-exec-4.0.0-SNAPSHOT.jar /spark/jars/hive-exec-1.2.1.spark2.jar

# Files that will be used by docker and must be copied inside the build folder
assure =
    ${spark_ball}
    hive-warehouse-connector-assembly-1.0.0-SNAPSHOT.jar
    up/${scala_file}

# Script to run inside docker.
run =
    ${hash}!/bin/bash -x
    source /common.sh

    export SPARK_DIST_CLASSPATH=$$(hadoop classpath)
    echo "export SPARK_DIST_CLASSPATH=$$(hadoop classpath)" > .bash_profile.sh
    until kinit -kt /var/keytabs/hdfs.keytab hdfs/hs2.example.com; do sleep 2; done
    wait_for_nn

    cp /hive/lib/calcite-core-* /spark/jars/
    cp /hive/lib/hbase-common-*.jar /spark/jars/
    cp /hive/lib/orc-core-*.jar /spark/jars/
    cp /hive/lib/orc-shims-*.jar /spark/jars/
    # cp /hive/lib/hive-llap-common-4.0.0-SNAPSHOT.jar /spark/jars/
    # cp /hive/lib/hive-llap-client-4.0.0-SNAPSHOT.jar /spark/jars/

    hdfs dfs -mkdir -p /user/spark_jars
    hdfs dfs -copyFromLocal /spark/jars/* /user/spark_jars/
    hdfs dfs -chmod -R 777 /user/hive/warehouse /user/spark_jars/

    cp /hive/lib/hive-common-4.0.0-SNAPSHOT.jar /spark/jars/
    # cp /hive/lib/hive-exec-4.0.0-SNAPSHOT.jar /spark/jars/
    cp /hive/lib/parquet-hadoop-bundle-1.10.0.jar /spark/jars/

    until kinit -kt /var/keytabs/hdfs.keytab hive/hs2.example.com; do sleep 2; done

    echo -e "spark.sql.hive.hiveserver2.jdbc.url \t\t jdbc:hive2://hs2.example.com:10000/;hive.server2.proxy.user=hive;" >> $$SPARK_HOME/conf/spark-defaults.conf
    echo -e "spark.datasource.hive.warehouse.load.staging.dir \t\t $$SPARK_HOME/tmp" >> $$SPARK_HOME/conf/spark-defaults.conf
    echo -e "spark.hadoop.hive.llap.daemon.service.hosts \t\t @llap0" >> $$SPARK_HOME/conf/spark-defaults.conf
    echo -e "spark.sql.hive.hiveserver2.jdbc.url.principal	 \t\t hive/hs2.example.com@EXAMPLE.COM" >> $$SPARK_HOME/conf/spark-defaults.conf
    echo -e "spark.security.credentials.hiveserver2.enabled \t\t false" >> $$SPARK_HOME/conf/spark-defaults.conf
    echo -e "spark.sql.hive.llap \t\t true" >> $$SPARK_HOME/conf/spark-defaults.conf
    echo -e "spark.hadoop.hive.llap.daemon.service.hosts \t\t @dhive-llap" >> $$SPARK_HOME/conf/spark-defaults.conf
    echo -e "spark.hadoop.hive.zookeeper.quorum \t\t zk1.example.com:2181" >> $$SPARK_HOME/conf/spark-defaults.conf

    echo -e "spark.hadoop.yarn.resourcemanager.hostname \t\t rm.example.com" >> $$SPARK_HOME/conf/spark-defaults.conf
    echo -e "spark.hadoop.yarn.resourcemanager.address \t\t rm.example.com:8032" >> $$SPARK_HOME/conf/spark-defaults.conf
    echo -e "spark.yarn.access.namenodes \t\t hdfs://nn.example.com:9000" >> $$SPARK_HOME/conf/spark-defaults.conf
    echo -e "spark.yarn.executor.memoryOverhead  \t\t 1GB" >> $$SPARK_HOME/conf/spark-defaults.conf
    echo -e "spark.driver.allowMultipleContexts  \t\t true" >> $$SPARK_HOME/conf/spark-defaults.conf
    echo -e "spark.streaming.receiver.maxRate  \t\t 900" >> $$SPARK_HOME/conf/spark-defaults.conf
    echo -e "spark.streaming.receiver.writeAheadLog.enable \t\t true" >> $$SPARK_HOME/conf/spark-defaults.conf

    ip_address=`ifconfig | grep -Eo 'inet (addr:)?([0-9]*\.){3}[0-9]*' | grep -Eo '([0-9]*\.){3}[0-9]*' | grep -v '127.0.0.1'`
    echo -e "spark.driver.host  \t\t $$ip_address" >> $$SPARK_HOME/conf/spark-defaults.conf

    sudo yum -y install nc

    pushd hive
    cat <<EOT >> create_file.py
    import sys

    if __name__ == "__main__":
    \twith open(sys.argv[2], "w") as f:
    \t\tfor i in range(int(sys.argv[1])):
    \t\t\tf.write("{0},{0}\n".format(i))

    EOT
    python create_file.py 1000 to_send.txt
    cat to_send.txt | nc -l localhost 1234 &

    ${hash} python create_file.py 1000 to_send.txt && cat to_send.txt | nc -l localhost 1234
    hdfs dfs -mkdir /tmp/dataspark
    ${hash} hdfs dfs -copyFromLocal to_send.txt /tmp/dataspark
    ${hash} hdfs dfs -rm  /tmp/dataspark/to_send.txt  && hdfs dfs -copyFromLocal to_send.txt /tmp/dataspark

    popd

    /sleep.sh
    ${hash} Wait for the metastore to open the port
    while :; do
        curl hm.example.com:9083 --connect-timeout 2;
        if [ $$? -ne 7 ]; then
            break
        fi
        echo "Waiting for the hive metastore to come up"; sleep 2;
    done

    hdfs dfs -mkdir /tmp/checkpoint
    hdfs dfs -chmod 777 /tmp/checkpoint

    sleep 120

    $$SPARK_HOME/bin/spark-shell \
        -i /stream_and_read.scala \
        --master yarn \
        --driver-memory 1024m \
        --executor-memory 1024m \
        --executor-cores 1 \
        --queue llap \
        --jars /hive-warehouse-connector-assembly-1.0.0-SNAPSHOT.jar

    $$SPARK_HOME/bin/spark-submit --class org.apache.spark.examples.SparkPi \
        --master yarn \
        --deploy-mode cluster \
        --driver-memory 512m \
        --executor-memory 512m \
        --executor-cores 1 \
        --queue llap \
        $$SPARK_HOME/examples/jars/spark-examples*.jar \
        10
    /sleep.sh
