# Sample config showing how to add a new module

[DEFAULT]


[global]
mysql_connector_version = 5.1.46

hadoop_version = 3.1.2

hive_version = 3.1.2
tez_version = 0.9.2

hive_path = /Users/jmarhuenda/workspace/hive
tez_path = /Users/jmarhuenda/workspace/tez


[core]

[hdfs]

[yarn]
#yarn.scheduler.maximum-allocation-mb=8096
yarn.scheduler.minimum-allocation-mb = 512
yarn.nodemanager.resource.cpu-vcores = 24

[tez]
tez.am.resource.memory.mb=512
tez.aux.uris=hdfs://nn.example.com:9000/apps/tez/
tez.runtime.io.sort.mb=256
tez.runtime.unordered.output.buffer.size-mb = 1024
tez.task.resource.memory.mb=512
tez.am.java.opts=-Xmx560m -Xms560m
tez.task.resource.cpu.vcores = 1

tez.shuffle-vertex-manager.max-src-fraction = 0
tez.shuffle-vertex-manager.min-src-fraction = 0
tez.shuffle-vertex-manager.enable.auto-parallel = true

tez.task-specific.log.level=INFO
tez.task.log.level=INFO
tez.am.log.level=INFO

[hive]

hive.server2.enable.doAs = true
hive.warehouse.subdir.inherit.perms = true

hive.metastore.kerberos.principal = hive/_HOST@EXAMPLE.COM
hive.tez.container.size = 512

#hive.security.authenticator.manager=org.apache.hadoop.hive.ql.secuassrity.SessionStateUserAuthenticator

[hivemetastore]

hive.security.metastore.authorization.manager = org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider
hive.security.metastore.authenticator.manager = org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator
hive.metastore.pre.event.listeners = org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener
hive.metastore.execute.setugi = true

hive.warehouse.subdir.inherit.perms = true
# hive.users.in.admin.role = hive,hive_meta

hive.stats.column.autogather = true
hive.insert.into.multilevel.dirs = true

hive.metastore.warehouse.external.dir = /user/hive/external
# hive.metastore.warehouse.external.dir =

[services]
# Order is important
# The module file will be kerberos.yml, hadoop.yml, ...
services = kerberos,hadoop,yarn,tez,external_jar

[external_jar]
hash = #
jar_to_run = performance-client-0.9.2.jar

# Lines to add the Dockerfile
docker =
    COPY ${jar_to_run} /

# Files that will be used by docker and must be copied inside the build folder
assure =
    ${jar_to_run}

# Script to run inside docker. Usually it will submit a jar to hadoop
# This script will be ran as user hdfs
run =
    ${hash}!/bin/bash -x
    until kinit -kt /var/keytabs/hdfs.keytab hdfs/nn.example.com; do sleep 2; done
    /populate-data.sh
    export HADOOP_CLASSPATH=$${TEZ_CONF_DIR}:$${TEZ_JARS}/*:$${TEZ_JARS}/lib/*
    hdfs dfs -rm -f /apps/tez/${jar_to_run}
    hdfs dfs -copyFromLocal ${jar_to_run} /apps/tez/
    hadoop jar ${jar_to_run} hdfs://nn.example.com:9000/user/random_user/people.txt
    /sleep.sh

# TODO
# kerberos user that need auth and might be used in the script in the run section.
# Not working now, best is to use one of the existing kerberos users
kerberos =
    hdfs
