# config showing how to enable spark

[global]

spark_version = 2.3.1
hadoop_version = 3.1.0
hive_version = 3.0.0
tez_version = 0.9.1

# hive_path = /Users/username/workspace/hive
# tez_path = /Users/username/workspace/tez


[core]

[hdfs]

[yarn]

[tez]

[hive]
# hive.root.logger = INFO,console

# To remove a present variable:
# hive.metastore.server.min.threads =

[services]
# Order is important
# The module file will be kerberos.yml, hadoop.yml, ...
services = kerberos,hadoop,yarn,tez,hive-meta,hive,external_spark

[external_spark]
hash = #
spark_version = 2.3.1
spark_ball = spark-${spark_version}-bin-hadoop2.7.tgz

# Lines to add the Dockerfile
docker =
    ADD ${spark_ball} /
    RUN ln -s spark-${spark_version}-bin-hadoop2.7 spark
    ENV SPARK_HOME=/spark

# Files that will be used by docker and must be copied inside the build folder
assure =
    ${spark_ball}

# Script to run inside docker.
run =
    ${hash}!/bin/bash -x
    until kinit -kt /var/keytabs/hdfs.keytab hdfs/nn.example.com; do sleep 2; done
    sleep 50
    $$SPARK_HOME/bin/spark-submit --class org.apache.spark.examples.SparkPi \
        --master yarn \
        --deploy-mode cluster \
        --driver-memory 512m \
        --executor-memory 512m \
        --executor-cores 1 \
        --queue thequeue \
        $$SPARK_HOME/examples/jars/spark-examples*.jar \
        10
    /sleep.sh

kerberos =
    hdfs
