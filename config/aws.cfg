# config showing how to run without authentication

[global]
mysql_connector_version = 5.1.46

hadoop_version = 3.1.2

hive_version = 3.1.2
tez_version = 0.9.2

hive_path = /Users/jmarhuenda/workspace/hive
tez_path = /Users/jmarhuenda/workspace/tez

kerberos = no_kerberos

[core]
hadoop.security.authentication =
hadoop.security.authorization = false
# fs.s3a.awsSecretAccessKey =
# fs.s3a.awsAccessKeyId =
# fs.s3a.secret.key =
# fs.s3a.access.key =
# fs.s3a.aws.credentials.provider = org.apache.hadoop.fs.s3a.SharedInstanceProfileCredentialsProvider,org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider,org.apache.hadoop.fs.s3a.BasicAWSCredentialsProvider,com.amazonaws.auth.EnvironmentVariableCredentialsProvider,org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider
fs.s3a.aws.credentials.provider = org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
hive.vectorized.use.vector.serde.deserialize = true
hive.vectorized.use.row.serde.deserialize = true

hadoop.security.credential.provider.path = jceks://hdfs@nn.example.com:9000/user/hive/s3.jceks

[hdfs]
dfs.namenode.kerberos.principal =
dfs.namenode.keytab.file =
dfs.namenode.kerberos.internal.spnego.principal =
dfs.datanode.keytab.file =
dfs.datanode.kerberos.principal =
dfs.web.authentication.kerberos.principal =
dfs.web.authentication.kerberos.keytab =

[yarn]
yarn.resourcemanager.principal =
yarn.resourcemanager.keytab =
yarn.nodemanager.principal =
yarn.nodemanager.keytab =
yarn.timeline-service.keytab =
yarn.timeline-service.principal =
mapreduce.job.jar = /hadoop/share/hadoop/tools/lib/hadoop-aws-3.1.0.jar

[tez]
tez.am.resource.memory.mb = 256
tez.runtime.io.sort.mb = 512

[hive]

hive.server2.authentication =
hive.server2.authentication.kerberos.principal =
hive.server2.authentication.kerberos.keytab =
hive.metastore.kerberos.keytab.file =
hive.metastore.kerberos.principal =
hive.metastore.sasl.enabled = false

hive.server2.enable.doAs = true
hive.warehouse.subdir.inherit.perms = true

hive.aux.jars.path = file:///hadoop/share/hadoop/tools/lib/hadoop-aws-3.1.0.jar,file:///hadoop/share/hadoop/tools/lib/aws-java-sdk-bundle-1.11.271.jar

[hivemetastore]

hive.security.metastore.authorization.manager = org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider
hive.security.metastore.authenticator.manager = org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator
hive.metastore.pre.event.listeners = org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener
hive.metastore.execute.setugi = true

hive.metastore.warehouse.external.dir = /user/hive/external

[services]
# Order is important
# The module file will be kerberos.yml, hadoop.yml, ...
services = hadoop,yarn,tez,mysql,hive-meta,hive,external_aws


[external_aws]
hash = #
bucket_name = test.dhive
secret_key = REPLACEME_SECRET
access_key = REPLACEME_ACCESS

docker =

assure =

run =
    ${hash}!/bin/bash -x
    source /common.sh
    export HADOOP_CLASSPATH=HADOOP_CLASSPATH:$$(find /hadoop/share/hadoop/tools/lib -name '*.jar')
    # Wait for name node to open the port
    wait_for_nn
    hadoop credential create fs.s3a.secret.key -value ${secret_key} -provider jceks://hdfs@nn.example.com:9000/user/hive/s3.jceks
    hadoop credential create fs.s3a.access.key -value ${access_key} -provider jceks://hdfs@nn.example.com:9000/user/hive/s3.jceks
    hadoop credential list -provider jceks://hdfs@nn.example.com:9000/user/hive/s3.jceks
    hdfs dfs -chmod 777 /user/hive/s3.jceks

    hdfs dfs -ls s3a://${bucket_name}/

    ${hash}beeline -u "jdbc:hive2://hs2.example.com:10000/;hive.server2.proxy.user=hive"
    ${hash}CREATE EXTERNAL TABLE tableTest (key STRING, value INT) LOCATION 's3a://${bucket_name}/';
    ${hash}INSERT INTO tableTest VALUES ("1", 1);
    /sleep.sh
